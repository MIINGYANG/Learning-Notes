#### 多智能体强化学习入门（一）——基础知识与博弈

---

单智能体强化学习环境稳定不变;多智能体强化学习环境是复杂的、动态的，面临问题

1. 维度爆炸：状态空间和联结动作空间随智能体数量指数增长
2. 目标奖励确定困难：每个智能体任务可能不同，相互影响
3. 不稳定性：同伴策略改变导致自身最优策略改变，影响收敛
4. 探索-利用：每个智能体的探索可能影响其他智能体的策略，导致难以收敛

博弈论基础：

- 矩阵博弈

  $V_i\left(\pi_1, \cdots, \pi_i, \cdots, \pi_n\right)$表示智能体i在联结策略$(\pi_1,...,\pi_n)$下的期望奖励

  定义1：纳什均衡

  > $V_i\left(\pi_1^*, \cdots, \pi_i^*, \cdots, \pi_n^*\right) \geq V_i\left(\pi_1^*, \cdots, \pi_i, \cdots, \pi_n^*\right), \forall \pi_i \in \Pi_i, i=1, \cdots, n\ \ \ \ (1)$
  >
  > 如果所有智能体都不能在仅改变自身策略的情况下获得更大奖励，则达到了纳什均衡
  >
  > 如果用$Q_i(a_1,...,a_n)$表示，则：
  >
  > $\sum_{a_1, \cdots, a_n \in A_1 \times \cdots \times A_n} Q_i\left(a_1, \cdots, a_n\right) \pi_1^*\left(a_1\right) \cdots \pi_i^*\left(a_i\right) \cdots \pi_n^*\left(a_n\right) \geq \sum_{a_1, \cdots, a_n \in A_1 \times \cdots \times A_n} \\ Q_i\left(a_1, \cdots, a_n\right) \pi_1^*\left(a_1\right) \cdots \pi_i\left(a_i\right) \cdots \pi_n^*\left(a_n\right), \forall \pi_i \in \Pi_i, i=1, \cdots,n$

  定义2：严格纳什均衡

  (1)式严格大于，则为严格纳什均衡

  定义3：完全混合策略

  一个策略对于智能体动作集中的所有动作的概率都大于0，则这个策略为一个完全混合策略

  定义4：纯策略

  智能体的策略对一个动作的概率分布为1，对其余的动作的概率分布为0，则这个策略为一个纯策略

- 两个智能体的矩阵博弈中的纳什均衡
  $$
  R_1=\left[\begin{array}{ll}
  r_{11} & r_{12} \\
  r_{21} & r_{22}
  \end{array}\right], \quad R_2=\left[\begin{array}{ll}
  c_{11} & c_{12} \\
  c_{21} & c_{22}
  \end{array}\right]
  $$
  第i个智能体的奖励矩阵$R_i$的元素$r_{xy}$表示第一个智能体采用动作x，第二个智能体采用动作y时第i个智能体获得的奖励，通常将第一个智能体定义为行智能体(行号对应第一个智能体采取的动作)，第二个智能体定义为列智能体(列号对应第二个智能体采取的动作)

  定义5：零和博弈

  两个智能体为完全竞争对抗关系，$R_1=-R_2$，只有一个纳什均衡点，即使有多种纳什均衡策略，但期望奖励相同

  定义6：一般和博弈

  包括完全对抗博弈、完全合作博弈及两者混合博弈，可能存在多个纳什均衡点

  策略：$\pi_i=\left(\pi_i\left(a_1\right), \cdots, \pi_i\left(a_{m_i}\right)\right)$，值函数$V_i$可表示为：$V_i=\pi_1 R_i \pi_2^T$(这里用奖励值代替Q值)

  纳什均衡策略可表示为：

  $V_i\left(\pi_i^*, \pi_{-i}^*\right) \geq V_i\left(\pi_i, \pi_{-i}^*\right), \forall \pi_i \in P D\left(A_i\right)$，-i表示另一个智能体

  若满足：$r_{l f}>r_{-l f}, c_{l f}>c_{l-f}$，则l,f为纯策略严格纳什均衡，-l,-f表示除了l,f的另一个策略

- 线性规划求解双智能体零合博弈

  $\max _{\pi_i} \min _{a_{-i} \in A_{-i}} \sum_{a_i \in A_i} Q_i^*\left(a_i, a_{-1}\right) \pi_i\left(a_i\right)$

  该公式表达为每个智能体最大化与对手博弈最差情况下的期望奖励值

*马尔可夫决策过程包括一个智能体与多个状态;矩阵博弈包括多个智能体与一个状态，随机博弈(stochastic game / Markov game)是马尔可夫决策过程与矩阵博弈的结合，具有多个智能体与多个状态*

静态博弈：没有状态s，没有状态转移，如矩阵博弈

阶段博弈：若干状态的阶段博弈组成随机博弈，是在一个固定状态的静态博弈，随机博弈中Q函数是该阶段博弈的奖励函数

重复博弈：智能体重复访问同一个状态的阶段博弈，访问同一个状态会收集其他智能体信息与奖励值，学习更好的Q函数和策略

**多智能体强化学习过程，就是找到每个状态的纳什均衡策略，将这些策略联合起来**

多智能体强化学习最优策略可写为：$(\pi_1^*,...,\pi_n^*),对任意s\in S,i=1,...,n$满足：

$V_i\left(s, \pi_1^*, \cdots, \pi_i^*, \cdots, \pi_n^*\right) \geq V_i\left(s, \pi_1^*, \cdots, \pi_i, \cdots, \pi_n^*\right), \forall \pi_i \in \Pi_i$

强化学习的Bellman公式可写为：
$$
\begin{aligned}
V_i^*(s) &=\sum_{a_1, \cdots, a_n \in A_1 \times \cdots \times A_n} Q_i^*\left(s, a_1, \cdots, a_n\right) \pi_1^*\left(s, a_1\right) \cdots \pi_n^*\left(s, a_n\right) \\
Q_i^*\left(s, a_1, \cdots, a_n\right) &=\sum_{s^{\prime} \in S} \operatorname{Tr}\left(s, a_1, \cdots, a_n, s^{\prime}\right)\left[R_i\left(s, a_1, \cdots, a_n, s^{\prime}\right)+\gamma V_i^*\left(s^{\prime}\right)\right]
\end{aligned}
$$
MARL的纳什策略可写为：
$$
\begin{aligned}
&\sum_{a_1, \cdots, a_n \in A_1 \times \cdots \times A_n} Q_i^*\left(s, a_1, \cdots, a_n\right) \pi_1^*\left(s, a_1\right) \cdots \pi_i^*\left(s, a_i\right) \cdots \pi_n^*\left(s, a_n\right) \geq\\
&\sum_{a_1, \cdots, a_n \in A_1 \times \cdots \times A_n} Q_i^*\left(s, a_1, \cdots, a_n\right) \pi_1^*\left(s, a_1\right) \cdots \pi_i\left(s, a_i\right) \cdots \pi_n^*\left(s, a_n\right)
\end{aligned}
$$
根据每个智能体奖励函数可对随机博弈进行分类，奖励函数相同为完全合作博弈，奖励函数取负号，为完全竞争博弈或零合博弈

---

#### 多智能体强化学习入门（二）——基础算法（MinMax-Q, NashQ, FFQ, WoLF-PHC）

随机博弈与多智能体强化学习不能完全等价，因为随机博弈假定每个状态奖励矩阵已知，不需要学习;多智能体强化学习需要与环境不断交互学习状态的奖励值函数，再学习到最优纳什策略，一般情况下模型的转移概率和奖励函数未知，需要用Q-learning的方式逼近状态值函数或动作-状态值函数

评价指标：

1. 合理性：对手使用恒定策略情况下，当前智能体能够学习并收敛到一个相对于对手策略的最优策略
2. 收敛性：其他智能体也使用学习算法时，当前智能体能够学习并收敛到一个稳定策略(系统中所有智能体使用相同学习算法)

针对应用，多智能体强化学习分为零和博弈算法和一般和博弈算法

- Minimax-Q算法

  应用：两个玩家的零和随机博弈

  第i个智能体的状态值函数：

  $V_i^*(s)=\max _{\pi_i(s, \cdot)} \min _{a_{-i} \in A_{-i}} \sum_{a_i \in A_i} Q_i^*\left(s, a_i, a_{-i}\right) \pi_i\left(s, a_i\right), i=1,2$

  $-i$ 表示智能体 $i$ 的对手。 $Q_i^*\left(s, a_i, a_{-i}\right)$ 为联结动作状态值函数, 若 $Q_i^*(\cdot)$ 已知, 我们可以直 接用线性规划求解出状态s处的纳什均衡策略。但是在多智能体强化学习中, $Q_i^*(\cdot)$ 是末知的, 所 以借用Q-learning中优秀的TD来更新逼近真实的 $Q_i\left(s, a_i, a_{-1}\right)$ 值。整个算法流程如下

![img](https://pic2.zhimg.com/v2-e907b291d3ec2e6dd62c96a86b4fd171_r.jpg)

缺点：

1. 第5步不断求解线性规划，导致学习速度降低，增加计算时间
2. 为求解第5步，智能体i需要知道所有智能体动作空间，在分布式系统中难以满足
3. 只满足收敛性，不满足合理性。该算法是一个对手独立算法（opponent-independent algorithm），如果对手使用的是较差策略，智能体不能根据对手的策略学习到一个更优的策略（被较差策略拖后腿），只能收敛到该博弈的纳什均衡策略

- Nash Q-Learning算法

  Nash Q-Learning将Minimax算法从两人零和博弈扩展为多人一般和博弈，使用二次规划求解纳什均衡点，*收敛条件：在每一个状态s的阶段博弈中，都能够找到一个全局最优点或者鞍点*

![img](https://pic4.zhimg.com/80/v2-5d50dc1f2ad874c22f10d5e796f64347_720w.webp)

缺点：

1. 二次规划时间非常耗时，降低了算法学习速度
2. 只满足收敛性，不满足合理性。只能收敛到纳什均衡策略，不能根据其他智能体的策略优化调剂自身策略

- Friend-or-Foe Q-Learning算法

  FFQ从Minimax-Q算法扩展而来，能够处理一般和博弈，对一个智能体i，将其它所有智能体分为两组，一组为i的friend帮助i最大化奖励回报，另一组为i的foe对抗i降低其奖励回报，这样将n智能体的一般和博弈转化为了两智能体的零和博弈，纳什均衡策略求解方法：
  $$
  \begin{array}{r}
  V_i(s)=\max _{\left.\pi_1(s, \cdot), \cdots, \pi_{n_1}(s,)\right)} \min _{0_1, \cdots, o_{n_2} \in O_1 \times \cdots \times O_{n_2}} \sum_{a_1, \cdots, a_{n_1} \in A_1 \times \cdots \times A_{n_1}} \\
  Q_i\left(s, a_1, \cdots, a_{n_1}, o_1, \cdots, o_{n_2}\right) \pi_1\left(s, a_1\right), \cdots, \pi_{n_1}\left(s, a_{n_1}\right)
  \end{array}
  $$
  ![img](https://pic1.zhimg.com/80/v2-2f4cff250568ca25853f1bc8e23e3b54_720w.webp)

  另一种利用Minimax-Q进行多人博弈的方法：两队零和博弈，每组各有一个leader控制该队智能体所有策略，获取的奖励值也是小组整体奖励值

  FFQ没有team leader，每个智能体选择自己动作学习自己策略获得自己奖励值，但为了更新Q值，每个智能体需要在每一步观测其他所有friend和foe的执行动作

  缺点：需要利用线性规划，算法整体学习速度会变慢

- WoLF Policy Hill-Climbing

  上述三种方法都要在学习中维护Q函数，假设每个智能体动作空间相同，则每个智能体都需要一个$|S|\cdot |A|^n$大小的空间存储Q值

  解决上述问题的方法：每个智能体只用知道自己的动作维护Q值函数，空间降到$|S|\cdot |A|$

  WoLF：当智能体做的比期望值好时小心缓慢调整参数，比期望值差加快调整参数

  PHC：增大能够得到最大累积期望的动作的选取概率

  ![img](https://pic4.zhimg.com/80/v2-8420ec197cb0516076725645a1359ab3_720w.webp)

  WoLF-PHC能够收敛到纳什均衡策略，当期它智能体采用某个固定策略时，也能收敛到一个目前状况下的最优策略而不是一个可能效果不好的纳什均衡策略(因为PHC能够改进策略)

  优点：

  1. 可变学习速率$\delta$，策略效果较差使用$\delta_l$，策略效果较好使用$\delta_w$，满足$\delta_l>\delta_w$
  2. 不用观测其他智能体策略、动作和奖励值，需要更少空间记录Q值
  3. 不需要线性规划或者二次规划求解纳什均衡，算法速度提高

  缺点：能收敛到最优策略，但没有收敛性证明

  ![img](https://pic2.zhimg.com/80/v2-a549b9cfd895898e7a4cc74d7432ce55_720w.webp)

---

#### 多智能体强化学习入门（三）——矩阵博弈中的分布式学习算法

分布式算法没有一个中心节点进行总体控制，每个智能体通过与环境交互自己学习自己的最优策略，在不知道全局信息的情况下将策略收敛到纳什均衡点，*核心难点在于如何只使用不完整信息情况下，使每个智能体的策略收敛到纳什均衡点*

矩阵博弈的学习算法分为两类：

1. 学习自动机

   通过与环境交互获得奖励修改动作空间中每个动作的概率分布，是完全分布式的算法，每个智能体只用直到自己的策略和奖励值，不需要环境信息和其他智能体信息，用元组$(A,r,p,U)$表示，A表示动作集，r是奖励值，p是动作集的概率分布，即学习的策略，U表示一个学习算法

   - $L_{R-I}$(linear reward-inaction)

     可用于n智能体矩阵博弈或双智能体零和博弈

     收敛条件：博弈只具有纯策略严格纳什均衡，只有满足收敛条件，所有智能体才能收敛到纳什均衡策略，第i个智能体策略更新公式：

     $p_c^i(k+1)=p_c^i(k)+\eta r^i(k)\left(1-p_c^i(k)\right) \quad a_c$ is current action at step $\mathrm{k}$ $p_j^i(k+1)=p_j^i(k)-\eta r^i(k) p_j^i(k) \quad$ for all $\mathrm{a}_{\mathrm{j}}^{\mathrm{i}} \neq \mathrm{a}_{\mathrm{c}}^{\mathrm{i}}$

     如上式所示, $k$ 为时刻; $p_c^i$ 是指智能体的动作分布, 上标是指智能体的编号, 下标是指智能体的 动作编号; $\eta$ 是学习速率; $r^i(k)$ 是指智能体在时刻 $\mathrm{k}$ 执行动作 $a_c$ 获得的奖励值

     ![img](https://pic3.zhimg.com/80/v2-0636867a9ecf1649ee5dd0266ee655be_720w.webp)

     注：只能在博弈只具有纯策略的纳什均衡点的时候可以收敛

   - $L_{R-P}$(linear reward-penalty)

     在$L_{R-I}$基础上加了一个罚函数

     收敛条件：博弈只有混合策略的纳什均衡

     更新公式：

     $p_c^i(k+1)=p_c^i(k)+\eta_1 r^i(k)\left[1-p_c^i(k)\right]-\eta_2\left[1-r^i(k)\right] p_c^i(k)$     $a_c$ is current action at step k
     $p_j^i(k+1)=p_j^i(k)-\eta_1 r^i(k) p_j^i(k)+\eta_2\left[1-r^i(k)\right]\left[\frac{1}{m-1}-p_j^i(k)\right] \quad$ for all a $\mathrm{a}_{\mathrm{j}}^{\mathrm{i}} \neq \mathrm{a}_{\mathrm{c}}^{\mathrm{i}}$

     满足：$0<\eta_1, \eta_2<1$ 和 $\eta_2<\eta_1$

     ![img](https://pic1.zhimg.com/80/v2-ec5233fa0088f91aa621ede60bcdb204_720w.webp)

     注：只能应用于只有混合策略纳什均衡的博弈中

2. 梯度提升

   - WoLF-IGA（WoLF与梯度提升结合）——根据累积奖励关于策略的梯度修正策略，目的是使更新后的策略能获得更大奖励值

     获胜或优秀策略的含义：当前策略的累积预期奖励大于当前玩家纳什均衡策略和其他玩家使用当前策略的累积预期奖励

     **只适用于双智能体双动作矩阵博弈**

     令 $p_1$ 表示智能体 1 选择第 1 个动作的概率, $1-p_1$ 表示智能体 1 选择第 2 个动作的概率; 同理 $q_1$ 表示智能体2选择第 1 个动作的概率, 同理 $1-q_1$ 表示智能体 2 选择第2个动作的概率, 其算法的核 心公式为
     $$
     \begin{aligned}
     p_1(k+1)=p_1(k)+\eta \alpha_1(k) \frac{\partial V_1\left(p_1(k), q_1(k)\right)}{\partial p_1} \\
     q_1(k+1)=q_1(k)+\eta \alpha_2(k) \frac{\partial V_2\left(p_1(k), q_1(k)\right)}{\partial q_1} \\
     \alpha_1= \begin{cases}\alpha_{\min } & \text { if } \mathrm{V}_1\left(\mathrm{p}_1(\mathrm{k}), \mathrm{q}_1(\mathrm{k})\right)>\mathrm{V}_1\left(\mathrm{p}_1^*, \mathrm{q}_1(\mathrm{k})\right) \\
     \alpha_{\max } & \text { otherwise }\end{cases} \\
     \alpha_2= \begin{cases}\alpha_{\min } & \text { if } \mathrm{V}_2\left(\mathrm{p}_1(\mathrm{k}), \mathrm{q}_1(\mathrm{k})\right)>\mathrm{V}_1\left(\mathrm{p}_1(\mathrm{k}), \mathrm{q}_1^*)\right. \\
     \alpha_{\max } & \text { otherwise }\end{cases}
     \end{aligned}
     $$
     $\eta$ 为学习步长, 需要足够小。 $\alpha_i$ 体现WoLF思想, 满足 $\alpha_{\max }>\alpha_{\min }$, 是一个可变的学习速率，根据当前策略的效果来调整学习快慢。 $V_i\left(p_1(k), q_1(k)\right)$ 表示在时刻k使用策略 $\left(p_1(k), q_1(k)\right)$ 获得的累积期望回报。 $\left(p_1^*(k), q_1^*(k)\right)$ 表示纳什均衡策略。

     难点：需要已知大量信息，包括自身奖励矩阵，其他玩家策略和自己的纳什均衡

     注：收敛条件为双智能体双动作一般和矩阵博弈，纳什均衡策略为纯策略或混合策略

   - Lagging Anchor

     **双智能体零和博弈**

     我们定义 $v=\left[p_1, p_2, \cdots, p_{m_1}\right]^T$ 是智能体 1 对于其 $m_1$ 个动作的概率分布, 即策略; 同理 $w=\left[q_1, q_2, \cdots, q_{m_2}\right]^T$ 为智能体 2 的策略。其策略迭代公式为
     $$
     \begin{aligned}
     v(k+1) &=v(k)+\eta P_{m_1} R_1 Y(k)+\eta \eta_d(\bar{v}(k)-v(k)) \\
     \bar{v}(k+1) &=\bar{v}(k)+\eta \eta_d(v(k)-\bar{v}(k)) \\
     w(k+1) &=w(k)+\eta P_{m_2} R_2 X(k)+\eta \eta_d(\bar{w}(k)-w(k)) \\
     \bar{w}(k+1) &=\bar{w}(k)+\eta \eta_d(w(k)-\bar{w}(k))
     \end{aligned}
     $$
     其中, $\eta$ 为学习步长, $\eta_d>0$ 定义为拉锚因子 (anchor drawing factor),
     $P_{m_i}=I_{m_i}-\frac{1}{m_i} \mathbf{1}_{m_i} \mathbf{1}_{m_i}^T$ 是一个用来维持策略 $v, w$ 归一化的矩阵。 $Y(k)$ 是一个单位向量, 若智能体2在时刻k采用第 $\mathrm{i}$ 个动作则 $Y(k)_i=1$ 第 $\mathrm{i}$ 个元素为 1 , 其余元素为 $0 ; X(k)$ 同理。 $\bar{v}, \bar{w}$ 为锚参数, 表示每个智能体的策略的加权平均, 是Lagging Anchor算法的核心。

     收敛条件：双智能体零和博弈中，只有完全混合策略

     该算法要用到每一个智能体的奖励矩阵$R_1,R_2$，严格来说不能算一个分布式算法，但是放宽条件以及智能体之间可以通信情况下是一个不错的算法

![img](https://pic2.zhimg.com/80/v2-2d0143f4ad909d358f47978926798a65_720w.webp)

*$L_{R-P}$只能用于2动作存疑*

---

#### 多智能体强化学习入门（四）——MADDPG算法

《Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments》将AC算法进行了一系列改进，使其能够适用于传统RL算法无法处理的复杂多智能体场景

传统RL算法面临的问题：每个智能体不断学习改进其策略，从每个智能体角度看，环境是动态不稳定的

MADDPG算法特征:

1. 通过学习得到的最优策略，在应用时只利用局部信息就能给出最优动作
2. 不需要知道环境的动力学模型以及特殊的通信需求
3. 该算法不仅能用于合作环境，也能用于竞争环境

MADDPG算法技巧：

1. 集中式训练，分布式执行
2. 改进了经验回放记录的数据，为了能够适用于动态环境，每一条信息由$\left(x, x^{\prime}, a_1, \cdots, a_n, r_1, \cdots, r_n\right)$ 组成, $x=\left(o_1, \cdots, o_n\right)$ 表示每个智能体的观测
3. 利用策略集合效果优化（policy ensemble）：对每个智能体学习多个策略，改进时利用所有策略的整体效果进行优化，以提高算法稳定性以及鲁棒性

MADDPG本质是DPG算法，允许每个智能体有自己的奖励函数，由于脱胎于DPG算法，动作空间可以使连续的

MADDPG技巧详解：

1. 针对随机策略，其策略梯度(SPG)：

   $\nabla_{\theta_i} J\left(\theta_i\right)=E_{s \sim \rho^\pi, a_i \sim \pi_i}\left[\nabla_{\theta_i} \log \pi_i\left(a_i \mid o_i\right) Q_i^\pi\left(x, a_1, \cdots, a_n\right)\right]$

   扩展到确定性策略$\mu_{\theta_i}$，梯度为(DPG)：

   $\nabla_{\theta_i} J\left(\mu_i\right)=E_{x, a \sim D}\left[\left.\nabla_{\theta_i} \mu_i\left(a_i \mid o_i\right) \nabla_{a_i} Q_i^\mu\left(x, a_1, \cdots, a_n\right)\right|_{a_i=\mu_i\left(o_i\right)}\right]$

   可以看到SPG与DPG十分类似，但$$\left.\nabla_{a_i} Q_i^\mu\left(x, a_1, \cdots, a_n\right)\right|_{a_i=\mu_i\left(o_i\right)}$$与$Q_i^\pi\left(x, a_1, \cdots, a_n\right)$的不同之处在于前者是对每个智能体建立值函数，极大解决了传统RL算法在Multi-agent领域的不足，critic的更新方法借鉴了DQN中TD与目标网络思想
   $$
   L\left(\theta_i\right)=E_{x, a, r, x^{\prime}}\left[\left(Q_i^\mu\left(x, a_1, \cdots, a_n\right)-y\right)^2\right]
   $$

   $$
   where\  \mathrm{y}=\mathrm{r}_{\mathrm{i}}+\left.\gamma \overline{\mathrm{Q}}_{\mathrm{i}}^{\mu^{\prime}}\left(\mathrm{x}^{\prime}, \mathrm{a}_1^{\prime}, \cdots, \mathrm{a}_{\mathrm{n}}^{\prime}\right)\right|_{\mathrm{a}_{\mathrm{j}}^{\prime}=\mu_{\mathrm{j}}^{\prime}\left(\mathrm{o}_{\mathrm{j}}\right)}
   $$

   $\bar{Q}_i^{\mu^{\prime}}$ 表示目标网络, $\mu^{\prime}=\left[\mu_1^{\prime}, \cdots, \mu_n^{\prime}\right]$ 为目标策略具有滞后更新的参数 $\theta_j^{\prime}$ 。其他智能体的策略可以采用拟合逼近的方式得到, 而不需要通信交互

   MADDPG的启发：如果知道所有智能体的动作，环境就是稳定的，即使策略在不断更新环境也是恒定的，因为模型动力学是稳定的

   $P\left(s^{\prime} \mid s, a_1, \cdots, a_n, \pi_1, \cdots, \pi_n\right)=P\left(s^{\prime} \mid s, a_1, \cdots, a_n\right)$$=P\left(s^{\prime} \mid s, a_1, \cdots, a_n, \pi_1^{\prime}, \cdots, \pi_n^{\prime}\right)$

2. 估计其他智能体策略

   1.中用到了其他智能体策略需要不断通信获取，可以放宽该条件对其他智能体策略进行估计实现，每个智能体维护n-1个策略逼近函数$\hat{\mu}_{\phi_i^j}$，表示第i个智能体对第j个智能体策略$\mu_j$的策略逼近，逼近代价为对数代价函数，并且加上策略的熵，代价函数可写为：

   $L\left(\phi_i^j\right)=-E_{o_j, a_j}\left[\log \hat{\mu}_{\phi_i^j}\left(a_j \mid o_j\right)+\lambda H\left(\hat{\mu}_{\phi_i^j}\right)\right]$

   最小化上述代价函数，就能得到对其他智能体策略的逼近，之前公式中的y可替换为：

   $y=r_i+\gamma \bar{Q}_i^{\mu^{\prime}}\left(x^{\prime}, \hat{\mu}_{\phi_i^j}^{\prime 1}\left(o_1\right), \cdots, \hat{\mu}_{\phi_i^j}^{\prime n}\left(o_n\right)\right)$

   在更新 $Q_i^\mu$ 之前, 利用经验回放的一个采样batch更新 $\hat{\mu}_{\phi_i^j}$

3. 策略集合优化（policies ensemble）

   多智能体强化学习一个顽固的问题是由于每个智能体的策略都在更新迭代导致环境针对一个特定的智能体是动态不稳定的。这种情况在竞争任务下尤其严重，经常会出现一个智能体针对其竞争对手过拟合出一个强策略。但是这个强策略是非常脆弱的，不是我们希望得到的，因为随着竞争对手策略的更新改变，这个强策略很难去适应新的对手策略

   解决方法：第i个智能体的策略$\mu_i$由一个具有K个子策略的集合构成，每个训练episode只使用一个子策略$\mu_{\theta_i^{(k)}}$，简写为$\mu_i^{(k)}$

   对每个智能体，最大化策略集合的整体奖励：

   $J_e\left(\mu_i\right)=E_{k \sim \operatorname{unif}(1, \mathrm{~K}), s \sim \rho^{\prime \prime}, a \sim \mu_i^{(k)}}\left[\sum_{t=0}^{\infty} \gamma^t r_{i, t}\right]$

   并且为每个子策略k构建一个记忆存储$D_i^{(k)}$，优化策略集合的整体效果，针对每一个子策略的更新梯度：

   $\nabla_{\theta_i^{(k)}} J_e\left(\mu_i\right)=\frac{1}{K} E_{x, a \sim D_i^{(k)}}\left[\left.\nabla_{\theta_i^{(k)}} \mu_i^{(k)}\left(a_i \mid o_i\right) \nabla_{a_i} Q^{\mu_i}\left(x, a_1, \cdots, a_n\right)\right|_{a_i=\mu_i^{(k)}\left(o_i\right)}\right]$

---

#### 附：强化学习——DRQN分析详解

传统DQN的局限性：

1. 经验数据存储的内存有限
2. 需要完整的观测信息

DRQN将DQN中全连接层替换为LSTM网络，当使用部分观测数据训练模型，完全观测数据评估模型，模型的效果与观测数据的完整性有关;如果反过来，使用完全观测数据进行训练，部分观测数据进行评估，DRQN效果下降小于DQN，循环网络在观测质量变化情况下，具有更强的适应性

如果在某些游戏中，4帧状态无法满足状态的表达，会使得整个系统不具有马尔可夫性（reward不仅与这帧画面有关，还与前若干帧有关），需要使用循环网络辅助记忆

在部分可观的情况下MDP变为POMDP(部分可观马尔可夫决策过程)，在POMDP中，如果对DQN引入RNN来处理不完全观测会取得较好的结果，DRQN相对DQN能够更好的处理缺失的信息

部分可观性：实际环境中智能体很少能获得完整的状态信息，失去了马尔可夫性，POMDP表示为$(S,A,P,R,\Omega,O)$，S,A,P,R分别表示状态、动作、状态转移概率、奖励，智能体在每一步不再接收状态$s_t$而是收到观测$o_t$，$o_t \sim \mathcal{O}\left(s_t\right)$，如果在POMDP中使用DQN不能很好的逼近Q函数，这是由于$Q(o, a \mid \theta) \neq Q(s, a \mid \theta)$，引入RNN的DRQN能够更好地处理部分可观的情况，能更好的逼近实际的$Q(s,a|\theta)$以至于学到更优秀的策略

*是不是POMDP怎么判别：看与后续获得奖励相关的状态能否完全表达，如果看不出来可以根据实验效果看Q值，如果收敛的不好定义为部分观测*

DRQN结构：

![img](https://pic3.zhimg.com/80/v2-1a3c5cd3d58d50d7bc75a6422d5b71ba_720w.webp)

LSTM的更新方式：

- Bootstrapped序列更新

  从经验回放内存中随机选取一次游戏过程 (episode) ，从这次游戏过程的开始一直学习到游戏结 束。在每一个时刻 $\mathrm{t}$, 其目标状态值还是通过目标网络 $\bar{Q}(\bar{\theta})$ 来获取。在一次游戏过程中，每一时刻LSTM隐含层的状态值从上一时刻继承而来

  优点：让LSTM学习一次游戏过程所有时间序列记忆，更有利于时序推理

  缺点：序列化采样学习违背DQN随机采样的策略（因为神经网络要求学习数据独立同分布，由于时序数据之间有马尔科夫性，则会损害神经网络的学习效果）

- Bootstrapped随机更新

  从经验回放内存中随机选取一次游戏过程 (episode) , 再在这个游戏过程中随机选择一个时刻 点, 再选择若干步进行学习 (可以是一步) 。在每一个时刻t, 其目标状态值还是通过目标网络 $\bar{Q}(\bar{\theta})$ 来获取。在每一次训练前需要将LSTM隐含层的状态置为 0 

  优点：更符合DQN随机采样策略

  缺点：需要每次更新前将LSTM隐含层状态置为0,损害了LSTM的记忆能力

**DRQN在信息逐渐缺失情况下，效果下降小于DQN，说明其对缺失信息更具鲁棒性**

---

#### 多智能体强化学习入门（五）——QMIX分析

QMIX特点：

1. 学习得到分布式策略
2. 本质是值函数逼近算法
3. 对于一个联合动作-状态只有一个总奖励值，不是每个智能体得到一个自己的奖励值，因此只适合于合作环境，不能用与竞争对抗环境
4. 采用集中式学习，分布式执行
5. 训练时借用全局状态信息提高算法效果
6. 设计一个神经网络来整合每个智能体的局部值函数得到联合动作值函数，VDN是直接求和
7. 每个智能体的局部值函数只需要自己的局部观测，选出累积期望奖励最大的动作执行
8. 算法使联合动作值函数与每个局部值函数的单调性相同，对局部值函数取最大动作也就是使联合动作值函数最大
9. 算法针对的模型是分布式多智能体部分可观马尔可夫决策过程（Dec-POMDP）

**单智能体强化学习拓展到MARL的核心问题：一个是如何学习联合动作值函数，该函数的参数会随着智能体数量的增加而指数增加，面临维度爆炸;另一个是学得了联合动作值函数后，如何通过联合值函数提取出一个优秀的分布式策略**

- Dec-POMDP

  是将POMDP扩展到多智能体系统，每个智能体局部观测信息$o_{i,t}$，动作$a_{i,t}$，系统状态为$s_t$

  每个智能体的（动作-观测历史）/（时序动作-观测记录）可表示为：$\tau_i=\left(a_{i, 0}, o_{i, 1}, \cdots, a_{i, t-1}, o_{i, t}\right)$

  （联合动作-观测历史）/（所有智能体时序动作-观测记录）：$\tau=\left(\tau_1, \cdots, \tau_n\right)$

  每个智能体的分布式策略$\pi_i(\tau_i)$，其值函数$Q_i\left(\tau_i, a_i ; \theta_i\right)$都是跟动作-观测历史$\tau_i$有关，而不是跟状态有关

- IQL

  给每个智能体执行一个Q-Learning算法，对每个智能体来说环境动态不稳定，算法无法收敛，在部分应用中有较好效果

- VDN

  $\tau=\left(\tau_1, \cdots, \tau_n\right)$为联合动作-观测历史，$\tau_i=\left(a_{i, 0}, o_{i, 1}, \cdots, a_{i, t-1}, o_{i, t}\right)$ 为动作-观测历史, $a=\left(a_1, \cdots, a_n\right)$ 表示联合动作，$Q_{tot}$为联合动作值函数，$Q_i\left(\tau_i, a_i ; \theta_i\right)$ 为智能体i的局部动作值函数，局部值函数只依赖每个智能体局部观测，VDN采用直接相加求和的方式：

  $Q_{t o t}=\sum_{i=1}^n Q_i\left(\tau_i, a_i, ; \theta_i\right)$

  虽然 $Q_i\left(\tau_i, a_i ; \theta_i\right)$ 不是用来估计累积期望回报的, 但是这里依然叫它为值函数。分布式的策略可 以通过对每个 $Q_i\left(\tau_i, a_i ; \theta_i\right)$ 取max得到。

- DRQN

  采用LSTM替换DQN卷积层后的一个全连接层，达到记忆历史状态的作用，在部分可观情况下提高算法性能，QMIX解决的是多智能体POMDP问题，每个智能体采用的是DRQN算法

- QMIX

  VDN只是将每个智能体的局部动作值函数相加得到联合动作值函数，虽然满足联合值函数与局部值函数单调性相同可以进行分布化策略的条件，*但其没有在学习时利用状态信息以及没有采用非线性方式对单智能体局部值函数进行整合*，使得VDN算法还有很大提升空间

  算法特点：采用一个混合网络对单智能体局部值函数进行合并，并在训练学习过程中加入全局状态信息辅助，提高算法性能
  $$
  \operatorname{argmax}_u Q_{t o t}(\tau, u)=\left(\begin{array}{c}
  \operatorname{argmax}_{u_1} Q_1\left(\tau_1, u_1\right) \\
  \vdots \\
  \operatorname{argmax}_{u_n} Q_n\left(\tau_n, u_n\right)
  \end{array}\right)      (1)
  $$
  分布式策略就是贪心的通过局部$Q_i$获取最优动作，QMIX将上式转化为一种单调性约束：

  $\frac{\partial Q_{\text {tot }}}{\partial Q_i} \geq 0, \forall i \in\{1,2, \cdots, n\}$

  若满足以上单调性，则（1）成立，为实现上述约束，QMIX采用混合网络：

  ![img](https://pic2.zhimg.com/80/v2-d03d9d93cb31a14a43ff5956528e5159_720w.webp)

  图(c)表示每个智能体采用一个DRQN来拟合自身的Q值函数的到 $Q_i\left(\tau_i, a_i ; \theta_i\right)$, DRQN循环输入当前的观测 $o_{i, t}$ 以及上一时刻的动作 $a_{i, t-1}$ 来得到 $\mathbf{Q}$ 值。

  图(b)表示混合网络的结构。其输入为每个DRQN网络的输出。为了满足上述的单调性约束, 混合网 络的所有权值都是非负数，对偏移量不做限制，这样就可以确保满足单调性约束。

  为了能够更多的利用到系统的状态信息 $s_t$, 采用一种超网络 (hypernetwork), 将状态 $s_t$ 作为 输入, 输出为混合网络的权值及偏移量。为了保证权值的非负性, 采用一个线性网络以及绝对值激 活函数保证输出不为负数。对偏移量采用同样方式但没有非负性的约束, 混合网络最后一层的偏移 量通过两层网络以及ReLU激活函数得到非线性映射网络。由于状态信息 $s_t$ 是通过超网络混合到 $Q_{t o t}$ 中的, 而不是仅仅作为混合网络的输入项, 这样带来的一个好处是, 如果作为输入项则 $s_t$ 的 系数均为正, 这样则无法充分利用状态信息来提高系统性能, 相当于舍弃了一半的信息量。

  QMIX最终代价函数：

  $L(\theta)=\sum_{i=1}^b\left[\left(y_i^{\text {tot }}-Q_{t o t}(\tau, a, s ; \theta)\right)^2\right]$

  更新用到了传统的DQN的思想, 其中b表示从经验记忆中采样的样本数量, $y^{t o t}=r+\gamma \max _{a^{\prime}} \bar{Q}\left(\tau^{\prime}, a^{\prime}, s^{\prime} ; \bar{\theta}\right), \bar{Q}\left(\tau^{\prime}, a^{\prime}, s^{\prime} ; \bar{\theta}\right)$ 表示目标网络，r是全局共用奖励函数

  由于满足上文的单调性约束, 对 $Q_{t o t}$ 进行 $a r g m a x$ 操作的计算量就不在是随智能体数量呈指数 增长了，而是随智能体数量线性增长, 极大的提高了算法效率。

---

#### 多智能体强化学习入门（六）——MFMARL算法(Mean Field Multi-Agent RL)

难点：状态空间和动作空间随智能体数量的增多而迅速扩大，给计算以及探索带来了非常大的困难

MFMARL对多智能体系统给出了一个近似假设：对某个智能体，其他所有智能体对其产生的作用可以用一个均值替代，将一个智能体与其他智能体之间的相互作用简化为两个智能体之间的相互作用（该智能体与其所有邻居的均值），极大简化了智能体数量带来的模型空间的增大

Mean Field + Q-Learning(MF-Q)和Mean Field + AC(MF-AC)

特点：理论上给出了收敛性证明，能够收敛到纳什均衡点;可用于竞争或合作环境;每个智能体不知道环境的模型以及奖励模型，但是能够观察邻居智能体的动作与奖励;每个智能体有自己的奖励值

MFMARL将值函数$Q_\pi^j(s, a)$转化为只包含邻居之间相互作用的形式

$Q_j(s, a)=\frac{1}{N_j} \sum_{k \in N(j)} Q_j\left(s, a_j, a_k\right)$

$N(j)$表示智能体j邻居智能体的标签集，$N_j=|N(j)|$ 表示邻居节点的个数，对智能体之间的交互作用做了近似，保留了部分主要的交互作用（近似保留邻居之间的交互，去掉了非邻居之间的交互），对联合动作a做了近似化简，状态信息s仍是全局信息

MFT(假定所有智能体都是同构的，动作空间相同且离散)，每个智能体动作采用one-hot编码，邻居k的one-hot编码动作$a_k$ 可以表示为 $\bar{a}_j$ 与一个波动 $\delta a_{j, k}$ 的形式，$\bar{a}_j$ 为智能体 $j$ 邻居 $N(j)$ 的平均动作

$a_k=\bar{a_j}+\delta a_{j, k}, \quad$ where $\bar{a}_j=\frac{1}{N_j} \sum_k a_k$

把$a_k$代入$Q_j(s,a)$，通过泰勒展开，得到结论，如果值函数$Q_j\left(s, a_j, a_k\right)$ 是一个M-smooth函数（M阶导数连续），则

$Q_j(s,a) \approx Q_j(s,a_j,\bar{a_j})$

将多智能体简化为了一个中心智能体j与一个虚拟智能体$\bar{a_j}$相互作用，虚拟智能体是智能体j所有邻居作用效果的平均，得到MF-Q函数$Q_j(s,a_j,\bar{a_j})$，假设有一段经验$\left[s,\left\{a_j\right\},\left\{r_j\right\}, s^{\prime}\right]$，MF-Q通过下式循环更新：

$Q_{j, t+1}\left(s, a_j, \bar{a}_j\right)=(1-\alpha) Q_{j, t}\left(s, a_j, \bar{a}_j\right)+\alpha\left[r_j+\gamma v_{j, t}\left(s^{\prime}\right)\right]\\=Q_{j,t}(s,a_j,\bar{a}_j)+\alpha[r_j+\gamma v_{j,t}(s')-Q_{j,t}(s,a_j,\bar{a}_j)]$

MF-v函数$v_{j,t}(s')$可以定义如下：

$v_{j, t}\left(s^{\prime}\right)=\sum_{a_j} \pi_{j, t}\left(a_j \mid s, \bar{a}_j\right) E_{\bar{a}_j\left(a_{-j} \sim \pi_{-j, t}\right)}\left[Q_{j, t}\left(s^{\prime}, a_j, \bar{a}_j\right)\right]$

在每一时刻的阶段博恋中, $\bar{a}_j$ 是通过上一时刻邻居 $k$ 的策略 $\pi_{k, t}$ 得出的, 其策略参数中的 $\bar{a}_{k-}$ 也是使用的上一时刻的平均动作, 更新过程如下
$\bar{a}_j=\frac{1}{N_j} \sum_k a_k, \quad$ where $a_k \sim \pi_{k, t}\left(\cdot \mid s, \bar{a}_{k-}\right)$

通过上式可以计算出邻居平均动作 $\bar{a}_j$, 然后通过玻尔兹曼分布得到新的策略如下形式
$$
\pi_{j, t}\left(a_j \mid s, \bar{a}_j\right)=\frac{\exp \left(-\beta Q_{j, t}\left(s, a_j, \bar{a}_j\right)\right)}{\sum_{a j \in A_j} \exp \left(-\beta Q_{j, t}\left(s, a_j, \bar{a}_j\right)\right)}
$$

- MF-Q:

![img](https://pic4.zhimg.com/80/v2-adff0c5ccd84498b96e50818b85117c7_720w.webp)

- MF-AC

SPG（随机梯度下降）输出的是动作的概率函数，DPG（确定梯度下降）输出的是确定动作

actor策略梯度公式：

$\nabla_{\theta_j} L\left(\theta_j\right)=\left.\nabla_{\theta_j} \log \pi_{\theta_j}(s) Q_{\phi_j}\left(s, a_j, \bar{a}_j\right)\right|_{a=\pi_{\theta_j}(s)}$

critic类似于MF-Q更新：

![img](https://pic3.zhimg.com/80/v2-e2ded84b7c18435a2caea90477f22276_720w.webp)

优点：将智能体数量扩张时联合动作维度爆炸缩减为$\left[a_j, \bar{a}_j\right]$，由于每个智能体策略还是需要全局状态信息s，不算是一个分布化算法，依赖通信获取邻居智能体动作$a_k$

---

#### 多智能体强化学习入门（八）——图卷积强化学习DGN

*论文：GRAPH CONVOLUTIONAL REINFORCEMENT LEARNING*

多智能体环境具有高度动态性，如何学习两个智能体之间的交互关系？

- 利用图卷积处理智能体邻居数量不确定，随卷积层增加，智能体感知范围扩大
- 关系核（多头注意力机制）对智能体之间的交互建模
- 时序关系正则化确保交互关系的一致性，让学习更稳定

方法：将环境建模为图网络，每个节点为一个智能体，节点特征为智能体观测到的状态，每个智能体与邻居之间连接一条边。每层图网络利用attention机制对邻居节点信息加权，更新自己状态，随着网络层数增加，每个节点能获取信息的范围增大，用更新后节点信息计算Q值

图网络假设：每个智能体受邻居影响最多，距离越远影响越小，如果全部智能体交互将造成大量通信带宽与计算资源浪费且值函数对大量参数求导很困难，每个智能体只与自己的邻居交互，通信代价计算可接受

智能体i局部观测，节点特征表示为$o_i^t$, 动作表示为 $a_i^t$, 得到的奖励值为 $r_i^t$

DGN(Deep Graph Network)包括三个模块：观测编码模块，卷积层，Q值网络

1. 观测编码模块：每个节点观测 $o_i^t$ 通过一个全连接网络(MLP)编码为特征 $h_i^t$

   卷积层：远处节点的信息随着卷积层的增加传播到当前节点。每一层卷积层的数据最后叠加在一起输入Q网络中，因为每一层数据表示不同感知范围的信息

   Q网络：$\left(\mathcal{O}, \mathcal{A}, \mathcal{O}^{\prime}, \mathcal{R}, \mathcal{C}\right)$ 分别表示当前时刻观测集合 $\mathcal{O}=\left\{o_i, \ldots, o_n\right\}$, 动作集合, 下一时刻观测集合，奖励值集合，邻接矩阵集合，每采样S个样本进行网络更新，代价函数为

   $L(\theta)=\frac{1}{S} \sum_S \frac{1}{N} \sum_{i=1}^N\left(y_i-Q\left(O_{i, C}, a_i ; \theta\right)\right)^2$

   其中 $y_i=r_i+\gamma \max _{a^{\prime}} Q^{\prime}\left(O_{i, C}^{\prime}, a_i^{\prime} ; \theta^{\prime}\right), Q^{\prime}\left(\cdot ; \theta^{\prime}\right)$ 为target 网络, $O_{i, C}$ 就是各层卷积层输出的堆叠

   由于多智能体环境动态变化，邻接矩阵$\mathcal{C}$在两个时刻可能会改变导致网络收敛困难，文章假设两个时刻图网络连接关系不变，即邻接矩阵恒定，target网络更新方式为$\theta^{\prime}=\beta \theta+(1-\beta) \theta^{\prime}$

   DGN可看作一个中心化策略输出每个智能体动作，以优化平均回报，策略参数$\theta$在智能体间共享，但并不是完全共享，因为利用的是邻居的信息和对智能体之间的交互，也能学得复杂的合作策略

   ![img](https://pic4.zhimg.com/80/v2-47d9d6f6a8c07b5307ad8e872306eeb3_720w.webp)

2. 关系核：

   图网络节点更新方式没法更好的对邻居信息进行整合，文章引入multi-head attention mechanism在多种关系层面计算邻居节点对自身的影响，加权利用

   $\mathbb{B}_{+i}$ 表示 $\mathbb{B}_i+i$, 第 $m$ 个head (即第 $m$ 种关系) 下, 邻居 $j \in \mathbb{B}_i$ 的权重为
   $$
   \alpha_{i j}^m=\frac{\exp \left(\tau \cdot W_Q^m h_i \cdot\left(W_K^m h_j\right)^T\right)}{\sum_{k \in \mathbb{B}_{+i}}}
   $$
   $W_{Q / K}^m$ 为attention中的线性映射, 分别对应query、key。利用可学习的attention机制来捕获智 能体之间的交互关系, 并且每个head能捕获不同类型的交互关系。然后利用求出来关系权重 $\alpha^m$ 来对邻居信息进行加权处理。然后利用全连接网络以及ReLU非线性激活函数来作为卷积层的输出 $h_i^{\prime}=\sigma\left(\operatorname{concat}\left[\sum_{j \in \mathbb{B}+i} \alpha i j^m W_V^m h_j, \forall m \in M\right]\right)$

3. 时序关系正则化

   在实际的合作交互中，交互关系在短时间内是稳定不变的，该文章通过attention权重在短时间内的一致性实现，技巧是最小化当前时刻attention权重分布于下一时刻attention权重分布的KL散度实现，层数越高能够获得更大范围的感知域，学到的信息越多，文章对最后一层卷积层attention权重分布优化，**整个算法**代价函数：

   $L(\theta)=\frac{1}{S} \sum_S \frac{1}{N} \sum_{i=1}^N\left(y_i-Q\left(O_{i, C}, a_i ; \theta\right)\right)^2+\lambda \frac{1}{M}$
   $\sum_{m=1}^M D_{K L}\left(G_m^k\left(O_{i, C} ; \theta\right) \| G_m^k\left(O_{i, C}^{\prime} ; \theta\right)\right)$

---

#### 多智能体强化学习入门（九）——注意力图网络的多智能体博弈

*Multi-Agent Game Abstraction via Graph Attention Neural Network*

该文章采用两种注意力机制(two-stage attention network, G2ANet)

- 多智能体建模为图网络，即拓扑图，每个节点表示一个智能体，节点之间的边表示两个智能体间交互关系
- Hard-attention：断开多智能体拓扑图的边，只保留有交互的边，通常是通过采样得到，不能学习，该文章进行了改进能够端到端学习
- Soft-attention：对Hard-attention保留的交互，判断交互的重要性，会给每个邻居智能体分配一个权重，对无关智能体分配的权重非常小
- 将上述两种注意力机制与值函数强化学习与AC强化学习算法相结合

**总结：Hard-attention断开了无关交互边，Soft-attention确定交互边的重要性权重**

![img](https://pic3.zhimg.com/80/v2-998e5e9ec7852bc3b7fa31fdfa36fefa_720w.webp)

该文attention逻辑：

![img](https://pic1.zhimg.com/80/v2-a8fe9e1145be56adfe606301af57d3f4_720w.webp)

- Hard-attention

  输出为$W_h^{i, j} \in 0,1$ 表示是否有交互（是否存在边），用LSTM作为可学习的核：

  $W_h^{i, j}=g u m\left(f\left(B i-L S T M\left(h_i, h_j\right)\right)\right)$

  其中 $g u m(\cdot)$ 为gumbel-softmax函数, $f(\cdot)$ 表示一个全连接层, $h_i$ 表示节点 $i$ 的特征。由于 LSTM的输出与网络输入顺序有关, 并且 $W_h^{i, j}$ 只与 $\{1, \ldots, j\}$ 有关, 与之后的 $\{j+i, \ldots, n\}$ 无关, 为了避免这种情况, 文章采用Bi-LSTM函数, 两个LSTM函数分别从头尾两端开始计算, 每 次将正LSTM与反LSTM输出叠加进行hard-attention计算。

- Soft-attention

  采用最基础的attention机制处理

  $W_s^{i, j} \propto \exp \left(e_j^T W_k^T W_q e_i W_h^{i, j}\right)$

  $e_i$ 表示第 $i$ 个智能体的特征, $W_k, W_q$ 分别为key线性映射与query线性映射。最终得到每个边 $E_{i, j}$ 的重要性权重。

1. 基于G2ANet的策略网络

   将衍生出的该算法称为GA-Comm，是一种基于通信的算法，每个智能体能够通过通信得知邻居智能体的状态，如下：

   ![img](https://pic1.zhimg.com/80/v2-13b8b356f2f816a00f9db06bfa5b7fac_720w.webp)

   每个智能体观测为$o_i$，利用LSTM提取每个智能体特征：

   $h_i, s_i=\operatorname{LSTM}\left(e\left(o_i\right), h_i^{\prime}, s_i^{\prime}\right)$

   其中 $h_i, s_i$ 分别表示LSTM中的hidden state与cell state。 $e(\cdot)$ 为一个全连接层, 然后利用特征 $h_i$ 通过hard与soft attention得到连接权重

   $W_h^{i, j}=M_{\text {hard }}\left(h_i, h_j\right)$
   $W_s^{i, j}=M_{\text {soft }}\left(W_h, h_i, h_j\right)$

   最后利用soft-attention输出的权重对邻居特征进行加权处理得到邻居信息 $x_i$
   $$
   x_i=\sum_{j \neq i} w_j h_j=\sum_{j \neq i} W_h^{i, j} W_s^{i, j} h_j
   $$
   最后利用策略梯度算法REINFORECE得到每个智能体的策略，这里其实可以拓展到很多RL算法
   $$
   a_i=\pi\left(h_i, x_i\right)
   $$

2. 基于GA2Net的AC网络

   与上面的主要区别在于对每个智能体计算一个 $Q_i\left(o_i, a_i, x_i\right)=f_i\left(g_i\left(o_i, a_i\right), x_i\right)$, 其中 $f_i, g_i$ 是多层感知机, $x_i$ 为邻居信息的加权, 权重通过两种注意力机制计算得到

   $x_i=\sum_{j \neq i} w_j v_j=\sum_{j \neq i} w_j h\left(V g_j\left(o_j, a_j\right)\right)$

   其中 $h, V$ 为非线性函数与线性映射矩阵, $w_j=W_h^{i, j} W_s^{i, j}$





